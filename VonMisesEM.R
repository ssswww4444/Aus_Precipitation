#' Compare subsets of data against others
#'
#' Divide an angular data set into subsets and compare the characteristics of each subset against the remaining angles, to test whether they might plausibly share the same distribution. 
#' @param data Vector of angles.
#' @param clusts Vector of indices denoting the cluster to which each angle has been assigned.
#' @param include.noise Boolean indicator: should cluster 0 (noise cluster) be included in the output?
#' @return A matrix containing the size of the subset tested; the Rayleigh test score (\code{\link{rayleigh.test}}); Watson mean test score (\code{\link{watson.common.mean.test}}); Wallraff concentration test score (\code{\link{wallraff.concentration.test}});
#' Mardia-Watson-Wheeler large-sample test of common distribution (\code{\link{mww.common.dist.LS}}); and randomised Watson two-sample test of common distribution (\code{\link{same.dist.watson.rand}}).
#' @export
#' @examples
#' c.res <- compare.clusters(q, dbscan(centres, eps = 2.5, MinPts = 4)$cluster)
compare.clusters <- function(data, clusts, include.noise = F, reps = 999) {
  cr <- c()
  if (include.noise) {start <- 0} else {start <- 1}
  
  for (i in start:max(clusts)) {
    q4 <- data[clusts == i]
    q4.x <- data[clusts != i]
    qs <- list(q4, q4.x)
    ql <- c(length(q4), length(q4.x))
    
    cr <- rbind(cr, c(subset.size = length(q4),
                      rayleigh.unif = rayleigh.test(q4)$p.value,
                      rayleigh.unif.x = rayleigh.test(q4.x)$p.value,
                      same.mean = watson.common.mean.test(qs)$p.val,
                      same.conc = wallraff.concentration.test(qs)$p.val,
                      same.dist.mww.ls = mww.common.dist.LS(cs.unif.scores(qs), ql)$p.val,
                      same.dist.watson.rand = watson.two.test.rand(q4, q4.x, NR = reps, show.progress = F)$p.val))
  }
  rownames(cr) <- sort(unique(clusts))[(start + 1):length(unique(clusts))]
  cr
}


#' Sample statistics of subsets of data
#'
#' Divide an angular data set into subsets and obtain sample statistics for each subset. 
#' @param data Vector of angles.
#' @param clusts Vector of indices denoting the cluster to which each angle has been assigned.
#' @param include.noise Boolean indicator: should cluster 0 (noise cluster) be included in the output?
#' @return A matrix containing the index and size of the subset tested; sample statistics as generated by \code{bc.sample.statistics}; von Mises and Jones-Pewsey maximum-likelihood estimators; AIC and BIC scores for ML von Mises and Jones-Pewsey distributions.
#' @export
#' @examples
#' c.stats <- cluster.sample.stats(q, dbscan(centres, eps = 2.5, MinPts = 4)$cluster)
cluster.sample.stats <- function(data, clusts, include.noise = F) {
  
  cr <- c()
  if (include.noise) {start <- 0} else {start <- 1}
  
  for (i in start:max(clusts)) {
    q4 <- data[clusts == i]
    q4.x <- data[clusts != i]
    
    bc <- bc.sample.statistics(q4)
    bc.x <- bc.sample.statistics(q4.x)
    
    if (length(q4) < 5) {
      s <- c(id = i, size = length(q4), theta.bar = bc$mu, mean.res = bc$rho, bc.kappa = A1inv(bc$rho), kurtosis = bc$alpha2, vm.mu = NA, vm.kappa = NA, jp.mu = NA, jp.kappa = NA, jp.psi = NA, vm.AIC = NA, jp.AIC = NA, vm.BIC = NA, jp.BIC = NA)
      sx <- c(id = -i, size = length(q4.x), theta.bar = bc.x$mu, mean.res = bc.x$rho, bc.kappa = A1inv(bc.x$rho), kurtosis = bc.x$alpha2, vm.mu = NA, vm.kappa = NA, jp.mu = NA, jp.kappa = NA, jp.psi = NA, vm.AIC = NA, jp.AIC = NA, vm.BIC = NA, jp.BIC = NA)
    } else {
      vm <- mle.vonmises(q4)
      mle <- JP.mle(q4)
      
      vm.x <- mle.vonmises(q4.x)
      mle.x <- JP.mle(q4.x)
      
      comp <- JP.psi.info(q4)$comparison
      comp.x <- JP.psi.info(q4.x)$comparison
      
      s <- c(id = i, size = length(q4), theta.bar = bc$mu, mean.res = bc$rho, bc.kappa = A1inv(bc$rho), kurtosis = bc$alpha2, vm.mu = vm$mu %% (pi*2), vm.kappa = vm$kappa, jp.mu = mle$mu, jp.kappa = mle$kappa, jp.psi = mle$psi, vm.AIC = comp[4,1], jp.AIC = comp[4,2], vm.BIC = comp[5,1], jp.BIC = comp[5,2])
      sx <- c(id = -i, size = length(q4.x), theta.bar = bc.x$mu, mean.res = bc.x$rho, bc.kappa = A1inv(bc.x$rho), kurtosis = bc.x$alpha2, vm.mu = vm.x$mu %% (pi*2), vm.kappa = vm.x$kappa, jp.mu = mle.x$mu, jp.kappa = mle.x$kappa, jp.psi = mle.x$psi, vm.AIC = comp.x[4,1], jp.AIC = comp.x[4,2], vm.BIC = comp.x[5,1], jp.BIC = comp.x[5,2])
    }
    cr <- rbind(cr, s, sx)
  }
  rownames(cr) <- cr[,1]
  cr[,-1]
}


#' Sample statistics of subsets of data, per quadrant.
#'
#' Divide an angular data set into subsets and compare the characteristics of each subset against every other subset, to test whether they might plausibly share the same distribution. 
#' @param data Vector of angles.
#' @param clusts Vector of indices denoting the cluster to which each angle has been assigned.
#' @param include.noise Boolean indicator: should cluster 0 (noise cluster) be included in the output?
#' @return A matrix containing the size of the subset tested; the Rayleigh test score (\code{\link{rayleigh.test}}); Watson mean test score (\code{\link{watson.common.mean.test}}); Wallraff concentration test score (\code{\link{wallraff.concentration.test}});
#' Mardia-Watson-Wheeler large-sample test of common distribution (\code{\link{mww.common.dist.LS}}); and randomised Watson two-sample test of common distribution (\code{\link{same.dist.watson.rand}}).
#' @export
#' @examples
#' c.stats <- cluster.quad.tests(q, dbscan(centres, eps = 2.5, MinPts = 4)$cluster)
cluster.quad.tests <- function(data, clusts, include.noise = F, reps = 999) {
  cr <- c()
  rn <- c()
  qc <- c()
  
  if (include.noise) {start <- 0} else {start <- 1}
  
  # quadrants are based on modal angle from full data set
  mx <- circular(as.numeric(names(which.max(table(round(data, 1))))))
  cutpoints <- sort(circular(mx + c(pi/4, 3*pi/4, 5*pi/4, 7*pi/4)) %% (2*pi))
  quadrant <- rep(0, length(data))
  quadrant[data > cutpoints[1] & data < cutpoints[2]] <- 1
  quadrant[data > cutpoints[3] & data < cutpoints[4]] <- 1
  q.4 <- (4*data) %% (2*pi)
  
  for (i in start:max(clusts)) {
    q.4.a <- q.4[(quadrant == 0) & (clusts == i)]
    q.4.b <- q.4[(quadrant == 1) & (clusts == i)]
    
    bc.a <- bc.sample.statistics(q.4.a, symmetric = F)
    if (length(q.4.a) < 5) {
      sa <- c(size = length(q.4.a), theta.bar = bc.a$mu, mean.res = bc.a$rho, bc.kappa = A1inv(bc.a$rho), kurtosis = bc.a$alpha2, vm.mu = NA, vm.kappa = NA, jp.mu = NA, jp.kappa = NA, jp.psi = NA)
    } else {
      # get bias-corrected & ML point estimates
      vm <-  mle.vonmises(q.4.a, bias = T)
      vm$mu <- vm$mu %% (2*pi)
      jp <- JP.mle(q.4.a)
      
      sa <- c(size = length(q.4.a), theta.bar = bc.a$mu, mean.res = bc.a$rho, bc.kappa = A1inv(bc.a$rho), kurtosis = bc.a$alpha2, vm.mu = vm$mu %% (pi*2), vm.kappa = vm$kappa, jp.mu = jp$mu, jp.kappa = jp$kappa, jp.psi = jp$psi)
    }
    
    bc.b <- bc.sample.statistics(q.4.b, symmetric = F)
    if (length(q.4.b) < 5) {
      sb <- c(size = length(q.4.b), theta.bar = bc.b$mu, mean.res = bc.b$rho, bc.b.kappa = A1inv(bc.b$rho), kurtosis = bc.b$alpha2, vm.mu = NA, vm.kappa = NA, jp.mu = NA, jp.kappa = NA, jp.psi = NA)
    } else {
      # get bias-corrected & ML point estimates
      vm <-  mle.vonmises(q.4.b, bias = T)
      vm$mu <- vm$mu %% (2*pi)
      jp <- JP.mle(q.4.b)
      
      sb <- c(size = length(q.4.b), theta.bar = bc.b$mu, mean.res = bc.b$rho, bc.b.kappa = A1inv(bc.b$rho), kurtosis = bc.b$alpha2, vm.mu = vm$mu %% (pi*2), vm.kappa = vm$kappa, jp.mu = jp$mu, jp.kappa = jp$kappa, jp.psi = jp$psi)
    }
    
    if (length(q.4.a) < 5 | length(q.4.b) < 5) {
      qc.n <- c(rayleigh.unif.a = rayleigh.test(q.4.a)$p.value,
                rayleigh.unif.b = rayleigh.test(q.4.b)$p.value,
                same.mean = NA, same.conc = NA, same.dist.mww.ls = NA, same.dist.watson.rand = NA)
    } else {
      qs <- list(q.4.a, q.4.b)
      ql <- c(length(q.4.a), length(q.4.b))
      qc.n <- c(rayleigh.unif.a = rayleigh.test(q.4.a)$p.value,
                rayleigh.unif.b = rayleigh.test(q.4.b)$p.value,
                same.mean = watson.common.mean.test(qs)$p.val,
                same.conc = wallraff.concentration.test(qs)$p.val,
                same.dist.mww.ls = mww.common.dist.LS(cs.unif.scores(qs), ql)$p.val,
                same.dist.watson.rand = watson.two.test.rand(q.4.a, q.4.b, NR = reps, show.progress = F)$p.val)
    }    
    cr <- rbind(cr, sa, sb)
    rn <- rbind(rn, paste(i, "a", sep = ""), paste(i, "b", sep = ""))
    qc <- rbind(qc, qc.n)
  }
  rownames(cr) <- rn
  rownames(qc) <- sort(unique(clusts))[(start + 1):length(unique(clusts))]
  list(stats = cr, comparison = qc)
}


#' Compare all clusters for similar distribution
#'
#' Divide an angular data set into subsets, then subdivide those into perpendicular quadrants and obtain sample statistics for each subset. 
#' @param data Vector of raw angles (not quartered/wrapped angles).
#' @param clusts Vector of indices denoting the cluster to which each angle has been assigned.
#' @param reps Number of randomisation tests to apply
#' @return A three-dimensional array containing p-values for tests of similar mean, concentration and distribution.
#' @export
#' @examples
#' db <- dbscan(centres, eps = 2.5, MinPts = 4)$cluster
#' res <- compare.cluster.dists(q.4, db, reps = 99)
compare.cluster.dists <- function(data, clusts, reps = 999, show.progress = F) {
  n <- length(unique(clusts))
  res <- array(NA, dim = c(n,n,4), dimnames = list(c(unique(clusts)), c(unique(clusts)), c("mean", "conc", "dist.mww.LS", "dist.wats.rand")))
  
  if (show.progress) {pb <- txtProgressBar(min = 0, max = n, style = 3)}
  for (i in 1:max(clusts)-1) {
    for (j in (i+1):max(clusts)) {
      qi <- data[clusts == i]
      qj <- data[clusts == j]
      qs <- list(qi, qj)
      ql <- c(length(qi), length(qj))
      
      m <- watson.common.mean.test(qs)$p.val
      c <- wallraff.concentration.test(qs)$p.val
      d.mww <- mww.common.dist.LS(cs.unif.scores(qs), ql)$p.val
      d.wats.r <-  watson.two.test.rand(qi, qj, NR = reps, show.progress = F)$p.val
      
      res[i+1, j+1, 1] <- m
      res[j+1, i+1, 1] <- m
      
      res[i+1, j+1, 2] <- c
      res[j+1, i+1, 2] <- c 
      
      res[i+1, j+1, 3] <- d.mww
      res[j+1, i+1, 3] <- d.mww
      
      res[i+1, j+1, 4] <- d.wats.r
      res[j+1, i+1, 4] <- d.wats.r
      if (show.progress) {setTxtProgressBar(pb, j)}
    }
  }
  res
}


#' Plot cluster-by-cluster heatmap of p-values
#'
#' For a matrix of p-values, as generated by \code{\link{compare.cluster.dists}}, plot a heatmap highlighting p-values less than 0.05 or less than 0.1.
#' @param m1 Matrix of p-values to be plotted.
#' @param m2 Optional: Second matrix of p-values. If supplied, the lower triangular matrix of \code{m1} is overwritten with that of \code{m2} and the two are plotted together.
#' @export
#' @examples
#' cluster.pval.heatmap(res[,,1], res[,,2])
cluster.pval.heatmap <- function(m1, m2) {
  
  if (!missing(m2)) {m1[lower.tri(m1)] <- m2[lower.tri(m2)]}
  if (is.null(rownames(m1))) {rownames(m1) <- c(1:ncol(m1))}
  if (is.null(colnames(m1))) {colnames(m1) <- c(1:nrow(m1))}
  
  image(c(as.numeric(rownames(m1))), c(as.numeric(colnames(m1))), m1, col = c("orangered", "gold", "khaki1"), breaks = c(0, 0.05, 0.1, 1), xlab = "", ylab = "")
  
  for (x in 1:ncol(m1)) {
    xx <- colnames(m1)[x]
    abline(h = x - 0.5)
    for (y in 1:nrow(m1)) {
      yy <- as.numeric(colnames(m1)[y])
      text(xx, yy, round(m1[x,y],2), cex = 0.8)
      abline(v = y - 0.5)
    }
  }
}


#' Expectation-maximization algorithm for mixture of von Mises distributions
#'
#' For a vector of angles, will return the parameters of a mixture of von Mises distributions, using an EM algorithm.
#' @param x Vector of angles to be fitted
#' @param k Number of von Mises components to be fitted
#' @param max.runs Maximum number of iterations to attempt. Default is 1000.
#' @param conv Maximum difference in log-likelihood between successive iterations before convergence is considered to have occurred. Default is 0.00001.
#' @return List containing k, with k estimates of mu, kappa, alpha (proportion of population belonging to each component), log-likelihood found, number of iterations required, and first 10 iterations. Can be passed to \code{\link{plot.EM.vonmises}} to be displayed graphically.
#' @export
#' @examples
#' ex1 <- c(rvonmises(120 * 0.3, mu = circular(pi/2), kappa = 10),
#'          rvonmises(120 * 0.7, mu = circular(pi), kappa = 3))
#' em1 <- EM.vonmises(ex1, k = 2)
EM.vonmises <- function(x, k, max.runs = 1000, conv = 0.00001) {
  
  x <- circular(x)
  # provide starting values for mu, kappa, alpha
  mu <- circular(runif(k, 0, max(x)))
  kappa <- runif(k,0,1)
  alpha <- runif(k,0,1)
  alpha <- alpha/sum(alpha) # normalise to sum to 1
  
  # Support function - calculate log-likelihood
  
  log.likelihood <- function(x, mu, kappa, alpha, k) {
    l <- matrix(nrow = k, ncol = length(x))
    for (i in 1:k) {
      l[i,] <- alpha[i] * dvonmises(x, mu[i], kappa[i])
    }
    sum(log(colSums(l)))
  }
  
  log.lh <- log.likelihood(x, mu, kappa, alpha, k)
  new.log.lh <- abs(log.lh) + 100
  n = 0
  
  # create array to store initial values
  first.10 <- c(iter = n, mu = mu, kappa = kappa, alpha = alpha,
                log.lh = log.lh)
  
  while ((abs(log.lh - new.log.lh) > conv) && (n < max.runs)) {
    
    # Estimation - calculate z_ij
    z <- matrix(0, ncol = length(x), nrow = k)
    for (i in 1:k){
      z[i,] <- alpha[i] * dvonmises(x, mu[i], kappa[i])
    }
    all.z <- colSums(z)
    for (i in 1:k){
      z[i,] <- z[i,] / all.z
    }
    
    # Maximisation - update parameters
    for (i in 1:k) {
      alpha[i] <- sum(z[i,]) / length (x)
      mu[i] <- atan2(sum(z[i,] * sin(x)), sum(z[i,] * cos(x)))
      kappa[i] <- A1inv(sum(z[i,] * (cos(x - mu[i]))) / sum(z[i,]))
      
      # correct for negative kappa if necessary
      if (kappa[i] < 0) {
        kappa[i] <- abs(kappa[i])
        mu[i] <- mu[i] + pi
      }
    }
    
    # calculate log-likelihoods for comparison
    log.lh <- new.log.lh
    new.log.lh <- log.likelihood(x, mu, kappa, alpha, k)
    n <- n + 1
    
    # save first 10 iterations
    if (n < 11) {
      next.iter <- c(iter = n, mu = mu, kappa = kappa, alpha = alpha,
                     log.lh = new.log.lh)
      first.10 <- rbind(first.10, next.iter)
    }
  }
  
  # Output: if model hasn't converged, show error message
  #         if it has, output the parameters & first 10 iterations
  if ((abs(log.lh - new.log.lh) > conv)) {
    cat ("Data hasn't converged after", n, "iterations; \n",
         "Difference in log-likelihoods is", 
         round(abs(log.lh - new.log.lh),6))
  } else {
    row.names(first.10) <- first.10[,1]
    list(k = k, mu = mu %% (2*pi), kappa = kappa, alpha = alpha,
         log.lh = new.log.lh)
  }
}


#' Classify points using EM mixture model
#'
#' Winner-takes-all classification of points based on a von Mises mixture model obtained using \code{\link{EM.vonmises}}.
#' @param data Set of angular data
#' @param model Fitted mixture model from \code{\link{EM.vonmises}}.
#' @return Vector of cluster numbers
#' @export
#' @examples
#' ex1 <- c(rvonmises(120 * 0.3, mu = circular(pi/2), kappa = 10),
#'          rvonmises(120 * 0.7, mu = circular(pi), kappa = 3))
#' em1 <- EM.vonmises(ex1, k = 2)
#' cl <- EM.clusters(ex1, em1)
#' plot(ex1[cl == 1], stack = T)
#' points(ex1[cl == 2], col = "blue", stack = T)
EM.clusters <- function(data, model) {
  components <- matrix(nrow = model$k, ncol = length(data))
  for (i in 1:model$k) {
    components[i, ] <- dvonmises(data, circular(model$mu[i]), model$kappa[i])
  }
  apply(components, 2, which.max)
}


#' Plot mixture von Mises model found by EM algorithm.
#'
#' Plot original data with individual components and fitted mixture model.
#' @param varToPlot Set of angular data
#' @param modelToPlot Fitted mixture model from \code{\link{EM.vonmises}}.
#' @export
#' @examples
#' ex1 <- c(rvonmises(120 * 0.3, mu = circular(pi/2), kappa = 10),
#'          rvonmises(120 * 0.7, mu = circular(pi), kappa = 3))
#' em1 <- EM.vonmises(ex1, k = 2)
#' plot.EM.vonmises(ex1, em1)
plot.EM.vonmises <- function(varToPlot, modelToPlot, h.breaks = 20) {
  
  varToPlot = matrix(varToPlot)       # convert from circular data
  
  # get density of each component
  x <- circular(seq(0, 2*pi, 0.01))
  components <- matrix(nrow = modelToPlot$k, ncol = length(x))
  for (i in 1:modelToPlot$k) {
    components[i,] <- dvonmises(x, circular(modelToPlot$mu[i]), modelToPlot$kappa[i]) * modelToPlot$alpha[i]
  }
  mixt <- colSums(components)
  
  # plot original data
  y.max <- max(c(mixt, hist(varToPlot, plot = F, breaks = h.breaks)$density)) * 1.1 # rescale y axis
  labl <- paste("Mixture of", modelToPlot$k,"von Mises")
  hist(varToPlot, freq = F, ylim = c(0, y.max), main = "", col = "lightgrey", xlab = labl, xlim = c(0, 2*pi), breaks = h.breaks)
  
  # plot vM components
  for (i in 1:modelToPlot$k) {
    lines(matrix(x), components[i,], col = i+1, lwd = 2)
  }
  
  # add mixture model
  lines(matrix(x), mixt, lwd = 2)
}


#' Expectation-maximization algorithm for mixture of uniform and von Mises distributions
#'
#' For a vector of angles, will return the parameters of a mixture of von Mises distributions with one component constrained to be a continuous circular uniform distribution, using an EM algorithm.
#' @param x Vector of angles to be fitted
#' @param k Number of von Mises components to be fitted
#' @param max.runs Maximum number of iterations to attempt. Default is 1000.
#' @param conv Maximum difference in log-likelihood between successive iterations before convergence is considered to have occurred. Default is 0.00001.
#' @return List containing k, with k estimates of mu, kappa, alpha (proportion of population belonging to each component), log-likelihood found, number of iterations required, and first 10 iterations. Can be passed to \code{\link{plot.EM.vonmises}} to be displayed graphically.
#' @export
#' @examples
#' ex1 <- c(rvonmises(120 * 0.3, mu = circular(pi/2), kappa = 10),
#'          rvonmises(120 * 0.7, mu = circular(pi), kappa = 3))
#' em1 <- EM.vonmises(ex1, k = 2)
EM.u.vonmises <- function(x, k, max.runs = 1000, conv = 0.00001) {
  
  # E-M algorithm with one component fixed as uniform
  x <- circular(x)
  # provide starting values for mu, kappa, alpha
  mu <- circular(runif(k, 0, max(x)))
  kappa <- c(0, runif(k-1,0,1))
  alpha <- runif(k,0,1)
  alpha <- alpha/sum(alpha) # normalise to sum to 1
  
  # Support function - calculate log-likelihood
  
  log.likelihood <- function(x, mu, kappa, alpha, k) {
    l <- matrix(nrow = k, ncol = length(x))
    for (i in 1:k) {
      l[i,] <- alpha[i] * dvonmises(x, mu[i], kappa[i])
    }
    sum(log(colSums(l)))
  }
  
  log.lh <- log.likelihood(x, mu, kappa, alpha, k)
  new.log.lh <- abs(log.lh) + 100
  n = 0
  
  # create array to store initial values
  first.10 <- c(iter = n, mu = mu, kappa = kappa, alpha = alpha,
                log.lh = log.lh)
  
  while ((abs(log.lh - new.log.lh) > conv) && (n < max.runs)) {
    
    # Estimation - calculate z_ij
    z <- matrix(0, ncol = length(x), nrow = k)
    for (i in 1:k){
      z[i,] <- alpha[i] * dvonmises(x, mu[i], kappa[i])
    }
    all.z <- colSums(z)
    for (i in 1:k){
      z[i,] <- z[i,] / all.z
    }
    
    # Maximisation - update parameters
    for (i in 1:k) {
      alpha[i] <- sum(z[i,]) / length (x)
      mu[i] <- atan2(sum(z[i,] * sin(x)), sum(z[i,] * cos(x)))
      kappa[i] <- A1inv(sum(z[i,] * (cos(x - mu[i]))) / sum(z[i,]))   
      
      # correct for negative kappa if necessary
      if (kappa[i] < 0) {
        kappa[i] <- abs(kappa[i])
        mu[i] <- mu[i] + pi
      }
      
    }
    
    # Sort parameters by kappa for identifiability
    alpha <- alpha[order(kappa)]
    mu <- mu[order(kappa)]
    z <- z[order(kappa),]
    kappa <- kappa[order(kappa)]
    # fix smallest kappa as 0 (uniform)
    kappa[1] <- 0
    
    # calculate log-likelihoods for comparison
    log.lh <- new.log.lh
    new.log.lh <- log.likelihood(x, mu, kappa, alpha, k)
    n <- n + 1
    
    # save first 10 iterations
    if (n < 11) {
      next.iter <- c(iter = n, mu = mu, kappa = kappa, alpha = alpha,
                     log.lh = new.log.lh)
      first.10 <- rbind(first.10, next.iter)
    }
  }
  
  # Output: if model hasn't converged, show error message
  #         if it has, output the parameters & first 10 iterations
  if ((abs(log.lh - new.log.lh) > conv)) {
    cat ("Data hasn't converged after", n, "iterations; \n",
         "Difference in log-likelihoods is", 
         round(abs(log.lh - new.log.lh),6))
  } else {
    row.names(first.10) <- first.10[,1]
    list(k = k, mu = mu %% (2*pi), kappa = kappa, alpha = alpha,
         log.lh = new.log.lh)
  }
}


#' Mixture von Mises P-P plot
#'
#' Produces a P-P plot of the data against a specified mixture of von Mises distribution, to graphically assess the goodness of fit of the model.
#' @param data Vector of angles (in radians) to be fitted against the von Mises distribution.
#' @param mu1 Mean direction parameter for first von Mises component
#' @param kappa1 Concentration parameter for first von Mises component Must be between 0 and 1.
#' @param mu2 Mean direction parameter for second von Mises component
#' @param kappa2 Concentration parameter for second von Mises component Must be between 0 and 1.
#' @param prop Proportion of distribution assigned to first von Mises component. Must be between 0 and 1.
#' @return Vector of residuals.
#' @export
#' @examples
#' r.mvm <- rmixedvonmises(200, circular(pi), circular(0), kap, 0, prop)
#' m.pp.res <- mvM.PP(r.mvm, circular(pi),  kap, circular(0),0, prop)
#' mean(m.pp.res); sd(m.pp.res)
mvM.PP <- function(data, mu1, kappa1, mu2, kappa2, prop) {
  edf <- ecdf(data)
  tdf <- pmixedvonmises(data, mu1, mu2, kappa1, kappa2, prop, from = circular(0), tol = 1e-06)
  plot.default(tdf, edf(data), pch = 20, xlim = c(0, 1), ylim = c(0, 1), xlab = "mixture von Mises distribution function", ylab = "Empirical distribution function")
  lines(c(0, 1), c(0, 1), lwd = 2, col = "lightseagreen")
  edf(data) - tdf
}



#' Estimate stabilising parameter p for mean-shift clustering of circular data
#'
#' Automation of graphical method of estimating stabilising parameter to be used in mean-shift clustering algorithms. Calculates correlation of kernel density estimate for successive values of p, selecting as the optimum the maximum value of p that is less than 1 when rounded to a certain degree of precision.
#' @param data Set of angular data to be clustered.
#' @param dp How many decimal places of convergence is required before p is accepted?
#' @param plot Boolean: plot the correlation curve or not? Default is T.
#' @return Suggested value of p to be used in mean-shift clustering functions such as \code{\link{MSclust.NB}}
#' @export
#' @examples
#' ex1 <- c(rvonmises(120 * 0.3, mu = circular(pi/2), kappa = 10),
#'          rvonmises(120 * 0.7, mu = circular(pi), kappa = 3))
#' p <- MSclust.p.est(ex1, dp = 4)
MSclust.p.est <- function(data, dp = 4, plot = T) {
  
  MSBC.kde <- function(theta, data, p, beta) {
    
    b <- (1 - cos(theta - data))
    kf <- (1 - (b/beta))^p
    kf[b > beta] <- 0
    sum(kf)
  }
  
  # get kde for all data points
  beta <- sd.circular(data)
  kde <- list()
  for (j in 1:50) {
    kde[[j]] <- sapply(data, FUN = MSBC.kde, data = data, p = j, beta = beta)
  }
  p.corr <- c()
  for (j in 1:49) {
    p.corr[j] <- cor(kde[[j]], kde[[j+1]])
  }
  
  p <- length(p.corr[round(p.corr, dp) < 1])
  plot(c(1:49), p.corr, pch = 20, type = "o", xlab = "p", ylab = "Correlation between kde with p & p+1")
  abline(h = 1, col = "red")
  abline(v = p, col = "red")
  as.numeric(p)
}


#' Mean-shift clustering of circular data: non-blurring mean shift
#'
#' Iterative function to cluster angular data about an unspecified number of modes.
#' @param data Set of angular data to be clustered.
#' @param p Stabilising parameter p, estimated using \code{\link{MSclust.p.est}}.
#' @param conv.lim Required degree of convergence: when the degree of correlation between successive iterations is closer to 1 than this, the data is deemed to have converged. Default is 0.00001.
#' @return Matrix containing all iterations of the data set.
#' @export
#' @examples
#' ex1 <- c(rvonmises(120 * 0.3, mu = circular(pi/2), kappa = 10),
#'          rvonmises(120 * 0.7, mu = circular(pi), kappa = 3))
#' p <- MSclust.p.est(ex1, dp = 4)
#' nb <- MSclust.NB(ex1, p = 7, conv.lim = 0.000001)
MSclust.NB <- function(data, p, conv.lim = 0.00001, max.runs = 20) {
  n <- length(data)
  beta <- sd.circular(data)
  
  # set all data ponts as initial values of cluster centres
  vi <- matrix(data, nrow = 1)
  conv <- c(0)
  
  t <- 1
  while (conv[t] < (1-conv.lim) & t < max.runs) {
    vi.new <- c()
    for (i in 1:n) {
      kd <- apply(cbind(beta - (1 - cos(vi[t,i]-data)),0),1,max)^(p-1)
      upper <- kd * sin(data) / n
      lower <- kd * cos(data) / n
      vi.new[i] <- atan2(sum(upper), sum(lower)) %% (2*pi)
    }
    vi <- rbind(vi, t = vi.new)
    conv[t+1] <- cor(vi[t,], vi[t+1,])
    t <- t+1
  }
  vi
}


#' Summarise clusters from a mean-shift clustering
#'
#' Uses hierarchical clustering algorithm to extract mean, mean resultant length, and estimated kappa for each cluster.
#' @param clusts Vector or matrix of clustered values, as produced by \code{\link{MSclust.NB}}.
#' @return List containing a vector of cluster numbers and a matrix containing estimated mu, rho and kappa for each cluster.
#' @export
#' @examples
#' ex1 <- c(rvonmises(120 * 0.3, mu = circular(pi/2), kappa = 10),
#'          rvonmises(120 * 0.7, mu = circular(pi), kappa = 3))
#' p <- MSclust.p.est(ex1, dp = 4)
#' nb <- MSclust.NB(ex1, p = 7, conv.lim = 0.000001)
#' res <- MSclust.summ(nb)
MSclust.summ <- function(clusts, minPts = 5) {
  
  final <- clusts[nrow(clusts),]
  
  hcl <- hclust(dist(final), method = "single")
  clusters <- as.data.frame(cbind(cluster = cutree(hcl, h = mean(hcl$height)),
                                  org = clusts[1,], mode = final))
  cts <- count(clusters$cluster)
  res <- ddply(clusters, .(cluster), summarize,
               mu = mean.circular(circular(mode)) %% (2*pi),
               rho = rho.circular(circular(org)))
  res <- cbind(res, kappa = A1inv(res[,3]), n.points = count(clusters$cluster)$freq)
  list(summ = res, clusters = clusters$cluster)
}